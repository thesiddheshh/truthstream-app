# TruthStream: Real-Time Fake News Detection Pipeline

TruthStream is a production-ready, real-time fake news detection pipeline that integrates live data ingestion, preprocessing, classification using BERT-based models, verification through Wikidata, and visualization via a Streamlit dashboard. Built for robustness and scalability, the system combines Kafka, MongoDB, and RESTful APIs to deliver accurate, explainable insights into media credibility.

---

## ğŸ“ Project Structure

```
truthstream/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Raw JSON from Reddit and NewsAPI
â”‚   â”œâ”€â”€ labeled/                 # Cleaned & labeled training data
â”‚   â””â”€â”€ sources.md              # Documentation of data origins
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ bert_fake_news_classifier.pkl
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ reddit_stream.py       # Live Reddit posts via PRAW
â”‚   â”‚   â”œâ”€â”€ newsapi_fetch.py       # Fetch headlines via NewsAPI
â”‚   â”‚   â””â”€â”€ simulate_stream.py     # Local stream simulation
â”‚   â”‚
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”œâ”€â”€ kafka_producer.py      # Streams data to Kafka topic
â”‚   â”‚   â””â”€â”€ kafka_consumer.py      # Applies NLP pipeline on stream
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ clean_text.py          # Tokenization, stopwords, lemmatization
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ train_model.py         # Trains and saves BERT classifier
â”‚   â”‚   â””â”€â”€ predict.py             # Loads model and predicts
â”‚   â”‚
â”‚   â”œâ”€â”€ verification/
â”‚   â”‚   â”œâ”€â”€ verify_with_wikidata.py
â”‚   â”‚   â””â”€â”€ entity_linking.py      # (Optional) Named entity matching
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ save_to_mongo.py       # Inserts verified output to MongoDB
â”‚   â”‚   â””â”€â”€ schema_example.json
â”‚   â”‚
â”‚   â””â”€â”€ dashboard/
â”‚       â””â”€â”€ app.py                 # Streamlit frontend
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_exploration.ipynb
â”‚   â””â”€â”€ model_training.ipynb
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                # API keys, Kafka topics, model paths
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_cleaning.py
â”‚   â”œâ”€â”€ test_model.py
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ architecture.png               # Pipeline diagram
â”œâ”€â”€ README.md
â””â”€â”€ .env                           # API keys (excluded from git)
```

---

## ğŸš€ Features

- Real-time ingestion from Reddit and NewsAPI
- Modular NLP pipeline: cleaning, classification, verification
- Kafka streaming with producer-consumer architecture
- BERT-based fake news classifier with 90%+ test accuracy
- Optional entity linking with Wikidata for fact-checking
- Streamlit dashboard for user interaction and analytics
- MongoDB integration for storing verified results
- Notebooks for EDA and model experimentation
- Unit tests for pipeline stability

---

## âš™ï¸ Installation

### 1. Clone Repository

```bash
git clone https://github.com/yourusername/truthstream.git
cd truthstream
```

### 2. Set Up Python Environment

```bash
python -m venv venv
source venv/bin/activate      # Linux/Mac
venv\Scripts\activate         # Windows

pip install -r requirements.txt
```

### 3. Configure Environment

Create a `.env` file in the root directory:

```env
REDDIT_CLIENT_ID=your_id
REDDIT_SECRET=your_secret
NEWSAPI_KEY=your_key
MONGO_URI=mongodb://localhost:27017
```

Edit `config/config.yaml` to match your local paths and settings.

---

## ğŸ§  Model Training (Optional)

Use the provided notebook or script to train:

```bash
python src/model/train_model.py --data data/labeled/fake_news.csv --model_out models/bert_fake_news_classifier.pkl
```

---

## ğŸ›°ï¸ Running the Pipeline

### 1. Start Kafka and Zookeeper

Make sure Kafka and Zookeeper are running locally.

```bash
# Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# Start Kafka
bin/kafka-server-start.sh config/server.properties
```

### 2. Run Data Producers

```bash
python src/ingestion/reddit_stream.py
python src/ingestion/newsapi_fetch.py
```

### 3. Start Kafka Consumer + Classifier

```bash
python src/kafka/kafka_consumer.py
```

This script applies the NLP pipeline and pushes the output to MongoDB.

---

## ğŸ“Š Run the Dashboard

```bash
streamlit run src/dashboard/app.py
```

---

## ğŸ§ª Running Tests

```bash
pytest tests/
```

---

## ğŸ§± Tech Stack

- **Data Ingestion:** PRAW (Reddit), NewsAPI
- **Streaming:** Apache Kafka
- **Preprocessing:** NLTK, spaCy
- **Modeling:** BERT (Hugging Face Transformers)
- **Verification:** Wikidata SPARQL queries
- **Database:** MongoDB
- **Frontend:** Streamlit
- **Orchestration:** YAML, CLI scripts
- **Testing:** Pytest

---

## ğŸ“Œ Pipeline Architecture

![Architecture](architecture.png)

1. Live articles/comments ingested from Reddit & NewsAPI
2. Data streamed via Kafka topics
3. Kafka Consumer runs:
   - Text cleaning
   - BERT-based fake/real prediction
   - Optional fact-checking via Wikidata
4. Final output stored in MongoDB
5. Streamlit dashboard queries and displays results

---

## ğŸ“‚ MongoDB Schema

```json
{
  "source": "reddit" | "newsapi",
  "timestamp": "2025-06-13T15:30:00Z",
  "text": "...",
  "cleaned_text": "...",
  "prediction": "FAKE" | "REAL",
  "confidence": 0.92,
  "verified_entities": [...],
  "wikidata_verification": true
}
```

---

## ğŸ“ Future Enhancements

- Multilingual support (IndicBERT, XLM-R)
- Source trustworthiness scoring
- SHAP-based model explainability
- FastAPI backend integration
- Live alerts on misinformation trends

---

## ğŸ“œ License

MIT License Â© 2025 Siddheshwar Wagawad

---

## ğŸ‘¤ Contact

For questions or contributions, contact:  
**siddhwagawad@gmail.com**  
LinkedIn | GitHub | Portfolio

